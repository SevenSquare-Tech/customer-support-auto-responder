
import { PuppeteerWebBaseLoader } from "@langchain/community/document_loaders/web/puppeteer";
import * as puppeteer from "puppeteer"
import * as cheerio from "cheerio"
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { Document } from "langchain/document"
import { OpenAIEmbeddings } from "@langchain/openai";
import pc from "../../../../utils/pinecone";
import { v4 as uuidv4 } from "uuid";

const dataCleanUp = (pageContent: string) => {
  const $ = cheerio.load(pageContent);
  $("script, style").remove();
  const cleanedData = $.text();
  return cleanedData.replace(/[\x00-\x1F\x7F-\x9F]/g, "").trim();
};

export async function POST(request: Request) {
  try {
    const { url } = await request.json();


    const loader = new PuppeteerWebBaseLoader(url, {
      launchOptions: {
        headless: true,
      },
      async evaluate(page: puppeteer.Page, browser: puppeteer.Browser) {
        try {
      
          await page.goto(url);
      
          const textContent = await page.evaluate(() => {
            const bodyElement = document.querySelector("body");
            return bodyElement ? bodyElement.textContent : "";
          });
      
          await browser.close();
      
          return textContent || "";
        } catch (error) {
          console.error("Error occurred while loading the page: ", error);
          await browser.close();
          return "";
        }
      },
    });

    const docs = await loader.load();
    const pageContent = docs[0].pageContent;


    const cleanContent = dataCleanUp(pageContent);


    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000
      chunkOverlap: 50
    });


    const splitContent = await splitter.splitDocuments([
      new Document({ pageContent: cleanContent }),
    ]);


    const embeddings = new OpenAIEmbeddings({
      apiKey: process.env.OPEN_AI_KEY!,
      model: "text-embedding-3-large",
    });


    const index = pc.Index("ai-customer-support");

    /** 
     * Create embeddings for each chunk
     * vectorContent schema:
      [
        {
          "vector": [...],
          "text": "..."
        },
        ...
      ]
    */
    const vectorContent = await Promise.all(
  
      splitContent.map(async (doc) => {
    
        const vectors = await embeddings.embedDocuments([doc.pageContent]);
        const uniqueId = uuidv4();

    
        await index.upsert([
          {
            id: `${uniqueId}`,
            values: vectors[0],
            metadata: { text: doc.pageContent },
          },
        ]);

        return uniqueId;
      })
    );

    return Response.json({
      message: "Successful",
      content: vectorContent,
    });
  } catch (error: any) {
    console.error("Error in POST /api/rag:", error);
    return Response.json(
      { message: "Failed to scrape the website", error: error.message },
      { status: 500 }
    );
  }
}
